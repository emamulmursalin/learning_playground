{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a simple gradient boosting algorithm will be implemented to classify the well known basic NLP problem of \"Spam-Ham\" email classification. \n",
    "\n",
    "Different steps will be done sperately to show the process of preparing the data before feeding into the ML algorithm. New feature creation will took place and it's necessity wiil be evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                         email_text\n",
       "0    ham  I've been searching for the right words to tha...\n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3    ham  Even my brother is not like to speak with me. ...\n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header = None)\n",
    "data.columns = ['labels', 'email_text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>email_text</th>\n",
       "      <th>no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                         email_text  \\\n",
       "0    ham  I've been searching for the right words to tha...   \n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                             no_punc  \n",
       "0  Ive been searching for the right words to than...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "2  Nah I dont think he goes to usf he lives aroun...  \n",
       "3  Even my brother is not like to speak with me T...  \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's remove the punctuations from the email text and insert it as a new column\n",
    "#This join functions allows to get back the words as stacked string charecters\n",
    "\n",
    "def no_punc(text):\n",
    "    text_no_punc = ''.join(i for i in text if i not in st.punctuation)\n",
    "    return text_no_punc\n",
    "\n",
    "data['no_punc'] = data['email_text'].apply(no_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>email_text</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[Ive, been, searching, for, the, right, words,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[Even, my, brother, is, not, like, to, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                         email_text  \\\n",
       "0    ham  I've been searching for the right words to tha...   \n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [Ive, been, searching, for, the, right, words,...  \n",
       "1  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "2  [Nah, I, dont, think, he, goes, to, usf, he, l...  \n",
       "3  [Even, my, brother, is, not, like, to, speak, ...  \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we need to tokenize each word of the email text\n",
    "def tokenizing_text(text):\n",
    "    text_tokenize = re.split('\\W+', text)\n",
    "    return text_tokenize\n",
    "\n",
    "data['tokenized'] = data['no_punc'].apply(tokenizing_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many words in English which do not affect the NLP model positively and those are useless for the model. These are called stopwords and we need to remove it. It is to be mentioned that:\n",
    "\n",
    "    - stopwords in English are in lower case. So we need to make our email_text in lower case before comapering with stopwords\n",
    "    \n",
    "    - There are some stopwords which contain some punctuations. As we have already removed the punctuations, using stopword function after \"stemming\" or \"lemmatizing\" may results in better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>email_text</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[Ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[Even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                         email_text  \\\n",
       "0    ham  I've been searching for the right words to tha...   \n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Ive, been, searching, for, the, right, words,...   \n",
       "1  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "2  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [Even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]   \n",
       "\n",
       "                                         no_stopword  \n",
       "0  [ive, searching, right, words, thank, breather...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...  \n",
       "3  [even, brother, like, speak, treat, like, aids...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets load the stopwords of English language\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def removing_stopwords(text):\n",
    "    text_nostopword = [i.lower() for i in text if i.lower() not in stopword]\n",
    "    return text_nostopword\n",
    "\n",
    "data['no_stopword'] = data['tokenized'].apply(removing_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming means chopping off the extra characters from a word (e.g. \"played\" to \"play\"). It does not consider any neighbouring words before getting rid of extra letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>email_text</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopword</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[Ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[Even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                         email_text  \\\n",
       "0    ham  I've been searching for the right words to tha...   \n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Ive, been, searching, for, the, right, words,...   \n",
       "1  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "2  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [Even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]   \n",
       "\n",
       "                                         no_stopword  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [ive, search, right, word, thank, breather, pr...  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goe, usf, live, around, tho...  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To stemm the words, lets use the PorterStemmer from nltk. \n",
    "#(But according to the nltk.org website, Snowball stemmer is better for English Language)\n",
    "\n",
    "ps_stemmer = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text_stemmed = [ps_stemmer.stem(i) for i in text]\n",
    "    return text_stemmed\n",
    "\n",
    "data['stemmed'] = data['no_stopword'].apply(stemming)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lammetizing functions same like Stemming but it considers the meaning of the word, sorrounding words, parts of speech and some other factors. It is more accurate than Stemming but eventually slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>email_text</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopword</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatiezed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>Ive been searching for the right words to than...</td>\n",
       "      <td>[Ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, pr...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>Even my brother is not like to speak with me T...</td>\n",
       "      <td>[Even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                         email_text  \\\n",
       "0    ham  I've been searching for the right words to tha...   \n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  Ive been searching for the right words to than...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  Nah I dont think he goes to usf he lives aroun...   \n",
       "3  Even my brother is not like to speak with me T...   \n",
       "4                  I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Ive, been, searching, for, the, right, words,...   \n",
       "1  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "2  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [Even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]   \n",
       "\n",
       "                                         no_stopword  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [ive, search, right, word, thank, breather, pr...   \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goe, usf, live, around, tho...   \n",
       "3  [even, brother, like, speak, treat, like, aid,...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                         lemmatiezed  \n",
       "0  [ive, searching, right, word, thank, breather,...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, go, usf, life, around, though]  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will use Wordnet lemmatizer. \n",
    "#Thers are some other lemmatizers e.g. TextBlob, Stanford CoreNLP (requires Java installed) \n",
    "\n",
    "lem_wordnet = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmitization(text):\n",
    "    text_lemmitized = [lem_wordnet.lemmatize(i) for i in text]\n",
    "    return text_lemmitized\n",
    "\n",
    "data['lemmatiezed'] = data['no_stopword'].apply(lemmitization)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is the process of converting process text data into feature form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 8914)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  008704050406  0089my  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0  0  0             0       0     0            0            0           0   0   \n",
       "1  0  0             0       0     0            0            0           0   0   \n",
       "2  0  0             0       0     0            0            0           0   0   \n",
       "3  0  0             0       0     0            0            0           0   0   \n",
       "4  0  0             0       0     0            0            0           0   0   \n",
       "\n",
       "   020603  ...  zindgi  zoe  zogtorius  zoom  zouk  zyada  é  ü  üll  〨ud  \n",
       "0       0  ...       0    0          0     0     0      0  0  0    0    0  \n",
       "1       0  ...       0    0          0     0     0      0  0  0    0    0  \n",
       "2       0  ...       0    0          0     0     0      0  0  0    0    0  \n",
       "3       0  ...       0    0          0     0     0      0  0  0    0    0  \n",
       "4       0  ...       0    0          0     0     0      0  0  0    0    0  \n",
       "\n",
       "[5 rows x 8914 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets use a simple method which is Count vectorizing\n",
    "# It works on counting the occurance of each words \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=lemmitization)\n",
    "X_count = count_vect.fit_transform(data['no_stopword'])\n",
    "X_count_df = pd.DataFrame(X_count.toarray())\n",
    "X_count_df.columns = count_vect.get_feature_names()\n",
    "print(X_count.shape)\n",
    "X_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram vectorization method gives the flexibility to create bundle of one or more words and then calculate the occurance to create the feature vector. \n",
    "\n",
    "To use N-gram vectorization we need to add the words together again. So we have to write the cleaning process again. A compact apporach of data ceaning has been implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>email_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                         email_text  \\\n",
       "0  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "1   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "2   ham  Even my brother is not like to speak with me. ...   \n",
       "3   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4   ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "1           nah dont think go usf life around though  \n",
       "2      even brother like speak treat like aid patent  \n",
       "3                                        date sunday  \n",
       "4  per request melle melle oru minnaminunginte nu...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'email_text']\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text_no_punc = \"\".join([i.lower() for i in text if i.lower() not in st.punctuation])\n",
    "    text_tokenized = re.split('\\W+', text_no_punc)\n",
    "    text_lemmetized = \" \".join([lem_wordnet.lemmatize(i) for i in text_tokenized if i not in stopword])\n",
    "    return text_lemmetized\n",
    "\n",
    "data['preprocessed_text'] = data['email_text'].apply(text_preprocessing)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 31621)\n"
     ]
    }
   ],
   "source": [
    "#We will use of range of 2,2 which means it will only return the feature vectors for exactly\n",
    "    #two word's combination \n",
    "\n",
    "ngram_vect = CountVectorizer(ngram_range = (2,2))\n",
    "X_count_ngram = ngram_vect.fit_transform(data['preprocessed_text'])\n",
    "print(X_count_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So the number of columns (feature vectors) are much higher than found in Count vector method.\n",
    "#Because this time, same word's combination with different words has been counted separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF vectorizer uses the weight of each word instead of number of occurance. More the word is rare, the higher the weight will be.\n",
    "\n",
    "This vectorizer works almost similar to Count Vectorizer and does not require to pass the full sentence to pass as input. So we need to go back to our old data cleaning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'email_text']\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text_no_punc = \"\".join([i.lower() for i in text if i.lower() not in st.punctuation])\n",
    "    text_tokenized = re.split('\\W+', text_no_punc)\n",
    "    text_lemmetized = [lem_wordnet.lemmatize(i) for i in text_tokenized if i not in stopword]\n",
    "    return text_lemmetized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5567, 8911)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "lem_wordnet = nltk.WordNetLemmatizer()\n",
    "tfidf_vectorized = TfidfVectorizer(analyzer=text_preprocessing)\n",
    "X_tfidf_vectorized = tfidf_vectorized.fit_transform(data['email_text'])\n",
    "X_tfidf_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating features \n",
    "\n",
    "We can also create feature to improve the model. It is not necessary to do and also, not necessarily to be found useful. There will be two features to be experimented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>email_text</th>\n",
       "      <th>body_length</th>\n",
       "      <th>punc_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>49</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>62</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                         email_text  body_length  \\\n",
       "0  spam  Free entry in 2 a wkly comp to win FA Cup fina...          128   \n",
       "1   ham  Nah I don't think he goes to usf, he lives aro...           49   \n",
       "2   ham  Even my brother is not like to speak with me. ...           62   \n",
       "3   ham                I HAVE A DATE ON SUNDAY WITH WILL!!           28   \n",
       "4   ham  As per your request 'Melle Melle (Oru Minnamin...          135   \n",
       "\n",
       "   punc_perc  \n",
       "0       4.69  \n",
       "1       4.08  \n",
       "2       3.23  \n",
       "3       7.14  \n",
       "4       4.44  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'email_text']\n",
    "\n",
    "#Feature 01: Amount of non puncuation charecter in the email\n",
    "data['body_length'] = data['email_text'].apply(lambda x : len(x) - x.count(' '))\n",
    "\n",
    "#Feature 02: Percentage of non puncuation charecter in each email\n",
    "def punc_percentage(text):\n",
    "    count = sum([1 for i in text if i in st.punctuation]) \n",
    "    return round((count/(len(text) - text.count(' '))*100),2)\n",
    "data['punc_perc'] = data['email_text'].apply(punc_percentage)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_length</th>\n",
       "      <th>punc_perc</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8901</th>\n",
       "      <th>8902</th>\n",
       "      <th>8903</th>\n",
       "      <th>8904</th>\n",
       "      <th>8905</th>\n",
       "      <th>8906</th>\n",
       "      <th>8907</th>\n",
       "      <th>8908</th>\n",
       "      <th>8909</th>\n",
       "      <th>8910</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_length  punc_perc    0    1    2    3    4    5    6    7  ...  8901  \\\n",
       "0          128       4.69  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1           49       4.08  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2           62       3.23  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3           28       7.14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4          135       4.44  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "   8902  8903  8904  8905  8906  8907  8908  8909  8910  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8913 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's combine the new features with the feature vectors calculated from TFIDF vectorization\n",
    "\n",
    "X_features = pd.concat([data['body_length'], data['punc_perc'], \n",
    "                        pd.DataFrame(X_tfidf_vectorized.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement a simple Random forest classifier. It is less likely to be overfitted and works on voting methods. But we do not know the optimized parameter so we will use the Gridsearch to find the best parameter combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55.778557</td>\n",
       "      <td>0.084044</td>\n",
       "      <td>0.645684</td>\n",
       "      <td>0.060309</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 200}</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.973773</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>69.591669</td>\n",
       "      <td>7.549986</td>\n",
       "      <td>0.513887</td>\n",
       "      <td>0.136619</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.973413</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.268670</td>\n",
       "      <td>1.582302</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.059975</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 200}</td>\n",
       "      <td>0.980251</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.964061</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.972335</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.985775</td>\n",
       "      <td>0.345032</td>\n",
       "      <td>0.340631</td>\n",
       "      <td>0.048647</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 10}</td>\n",
       "      <td>0.970377</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.972158</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>58.420125</td>\n",
       "      <td>1.564029</td>\n",
       "      <td>0.680125</td>\n",
       "      <td>0.042289</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 300}</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.972156</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10      55.778557      0.084044         0.645684        0.060309   \n",
       "11      69.591669      7.549986         0.513887        0.136619   \n",
       "6       40.268670      1.582302         0.635856        0.059975   \n",
       "4        3.985775      0.345032         0.340631        0.048647   \n",
       "7       58.420125      1.564029         0.680125        0.042289   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "10            None                200   \n",
       "11            None                300   \n",
       "6               60                200   \n",
       "4               60                 10   \n",
       "7               60                300   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "10  {'max_depth': None, 'n_estimators': 200}           0.978456   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.978456   \n",
       "6     {'max_depth': 60, 'n_estimators': 200}           0.980251   \n",
       "4      {'max_depth': 60, 'n_estimators': 10}           0.970377   \n",
       "7     {'max_depth': 60, 'n_estimators': 300}           0.976661   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "10           0.975763           0.973944           0.964960   \n",
       "11           0.978456           0.973944           0.966757   \n",
       "6            0.973070           0.973944           0.964061   \n",
       "4            0.972172           0.973046           0.970350   \n",
       "7            0.975763           0.972147           0.964960   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "10           0.975741         0.973773        0.004636                1  \n",
       "11           0.969452         0.973413        0.004715                2  \n",
       "6            0.970350         0.972335        0.005257                3  \n",
       "4            0.974843         0.972158        0.001699                4  \n",
       "7            0.971249         0.972156        0.004145                5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameter_combos = {'n_estimators': [10, 100, 200, 300],\n",
    "                   'max_depth': [30, 60, None]}\n",
    "gs = GridSearchCV(rf, parameter_combos, cv = 5, n_jobs=-1)\n",
    "gs_fitted = gs.fit(X_features, data['label'])\n",
    "\n",
    "pd.DataFrame(gs_fitted.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So from gridsearch we can see that n_est = 200 and no bound to the max_depth performs best\n",
    "#Now first split the dataset into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 200 / Depth: None ---- Precision: 1.0 / Recall: 0.851 / Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "#Let's create a RF model and evaluate the performance\n",
    "from sklearn.metrics import precision_recall_fscore_support as scores\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=None, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = scores(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    200, 'None', round(precision, 3), round(recall, 3),\n",
    "    round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = 1 means when the classifiere classifies an email as \"spam\", it is 100% correct everytime\n",
    "\n",
    "Recall = 0.85 means when an email is spam, the classifier is 85% time correct to classify it as spam\n",
    "\n",
    "Accuracy = 0.98 means the classifier is overall 98% time correct for all the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try with Gradient boosting. It is similar to RandomForest to some extent. But it carries the learning from previous step so the process of building trees can not be parallelized. Which makes it slower. It is also easy to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347.168293</td>\n",
       "      <td>3.505811</td>\n",
       "      <td>0.213228</td>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.968582</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.97035</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.970899</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     347.168293      3.505811         0.213228        0.024696   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "0                 0.1              15                150   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.968582   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.976661            0.97035           0.966757           0.972147   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.970899        0.003394                1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [150], \n",
    "    'max_depth': [15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_features, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
